{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a9e6f0-7cfe-4894-9a8c-07870eed6807",
   "metadata": {},
   "source": [
    "# Setup the database with docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59901811-fd17-48c5-a7bd-edbbcf8921b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Master ETL Pipeline...\n",
      "\n",
      "üîå [Phase 1] Establishing Database Connection...\n",
      "‚úÖ Database 'lending_club' verified/created.\n",
      "‚öôÔ∏è Recreating table schema...\n",
      "\n",
      "‚öôÔ∏è [Phase 2] Running ETL (Extract -> Transform -> Load)...\n",
      "   -> Processing CSV Stream: /home/nanak/mlops/data/raw/loan.csv\n",
      "      -> Inserted 178 rows...\n",
      "      -> Inserted 388 rows...\n",
      "      -> Inserted 606 rows...\n",
      "      -> Inserted 823 rows...\n",
      "      -> Inserted 1039 rows...\n",
      "      -> Inserted 1294 rows...\n",
      "      -> Inserted 1593 rows...\n",
      "      -> Inserted 1855 rows...\n",
      "      -> Inserted 2163 rows...\n",
      "      -> Inserted 2450 rows...\n",
      "      -> Inserted 2793 rows...\n",
      "      -> Inserted 3162 rows...\n",
      "      -> Inserted 3527 rows...\n",
      "      -> Inserted 3948 rows...\n",
      "      -> Inserted 4394 rows...\n",
      "      -> Inserted 4908 rows...\n",
      "      -> Inserted 5415 rows...\n",
      "      -> Inserted 5984 rows...\n",
      "      -> Inserted 6610 rows...\n",
      "      -> Inserted 7186 rows...\n",
      "      -> Inserted 7840 rows...\n",
      "      -> Inserted 8523 rows...\n",
      "      -> Inserted 9299 rows...\n",
      "      -> Inserted 10087 rows...\n",
      "      -> Inserted 10925 rows...\n",
      "      -> Inserted 11847 rows...\n",
      "      -> Inserted 12741 rows...\n",
      "      -> Inserted 13686 rows...\n",
      "      -> Inserted 14722 rows...\n",
      "      -> Inserted 15745 rows...\n",
      "      -> Inserted 16883 rows...\n",
      "      -> Inserted 18128 rows...\n",
      "      -> Inserted 19352 rows...\n",
      "      -> Inserted 20618 rows...\n",
      "      -> Inserted 21918 rows...\n",
      "      -> Inserted 23296 rows...\n",
      "      -> Inserted 24698 rows...\n",
      "      -> Inserted 26111 rows...\n",
      "      -> Inserted 27632 rows...\n",
      "      -> Inserted 29238 rows...\n",
      "      -> Inserted 30874 rows...\n",
      "      -> Inserted 32486 rows...\n",
      "      -> Inserted 34157 rows...\n",
      "      -> Inserted 35938 rows...\n",
      "      -> Inserted 37789 rows...\n",
      "      -> Inserted 39685 rows...\n",
      "      -> Inserted 41779 rows...\n",
      "      -> Inserted 43845 rows...\n",
      "      -> Inserted 46045 rows...\n",
      "      -> Inserted 49861 rows...\n",
      "      -> Inserted 55669 rows...\n",
      "      -> Inserted 61475 rows...\n",
      "      -> Inserted 67409 rows...\n",
      "      -> Inserted 73302 rows...\n",
      "      -> Inserted 79231 rows...\n",
      "      -> Inserted 85096 rows...\n",
      "      -> Inserted 90956 rows...\n",
      "      -> Inserted 96911 rows...\n",
      "      -> Inserted 103095 rows...\n",
      "      -> Inserted 109348 rows...\n",
      "      -> Inserted 115527 rows...\n",
      "      -> Inserted 121255 rows...\n",
      "      -> Inserted 127470 rows...\n",
      "      -> Inserted 133680 rows...\n",
      "      -> Inserted 139748 rows...\n",
      "      -> Inserted 145860 rows...\n",
      "      -> Inserted 151976 rows...\n",
      "      -> Inserted 158253 rows...\n",
      "      -> Inserted 164706 rows...\n",
      "      -> Inserted 171283 rows...\n",
      "      -> Inserted 177808 rows...\n",
      "      -> Inserted 184130 rows...\n",
      "      -> Inserted 190601 rows...\n",
      "      -> Inserted 197245 rows...\n",
      "      -> Inserted 204183 rows...\n",
      "      -> Inserted 211242 rows...\n",
      "      -> Inserted 219214 rows...\n",
      "      -> Inserted 227481 rows...\n",
      "      -> Inserted 235864 rows...\n",
      "      -> Inserted 244353 rows...\n",
      "      -> Inserted 252944 rows...\n",
      "      -> Inserted 261577 rows...\n",
      "      -> Inserted 268883 rows...\n",
      "      -> Inserted 274077 rows...\n",
      "      -> Inserted 279223 rows...\n",
      "      -> Inserted 284309 rows...\n",
      "      -> Inserted 289472 rows...\n",
      "      -> Inserted 294827 rows...\n",
      "      -> Inserted 300043 rows...\n",
      "      -> Inserted 305422 rows...\n",
      "      -> Inserted 310886 rows...\n",
      "      -> Inserted 316413 rows...\n",
      "      -> Inserted 322185 rows...\n",
      "      -> Inserted 331040 rows...\n",
      "      -> Inserted 339795 rows...\n",
      "      -> Inserted 348502 rows...\n",
      "      -> Inserted 357119 rows...\n",
      "      -> Inserted 365865 rows...\n",
      "      -> Inserted 374556 rows...\n",
      "      -> Inserted 383181 rows...\n",
      "      -> Inserted 391990 rows...\n",
      "      -> Inserted 400719 rows...\n",
      "      -> Inserted 409469 rows...\n",
      "      -> Inserted 418225 rows...\n",
      "      -> Inserted 427059 rows...\n",
      "      -> Inserted 435850 rows...\n",
      "      -> Inserted 444616 rows...\n",
      "      -> Inserted 453332 rows...\n",
      "      -> Inserted 462057 rows...\n",
      "      -> Inserted 470751 rows...\n",
      "      -> Inserted 479502 rows...\n",
      "      -> Inserted 488316 rows...\n",
      "      -> Inserted 497137 rows...\n",
      "      -> Inserted 506036 rows...\n",
      "      -> Inserted 514962 rows...\n",
      "      -> Inserted 523940 rows...\n",
      "      -> Inserted 532851 rows...\n",
      "      -> Inserted 541750 rows...\n",
      "      -> Inserted 550670 rows...\n",
      "      -> Inserted 559564 rows...\n",
      "      -> Inserted 568488 rows...\n",
      "      -> Inserted 577468 rows...\n",
      "      -> Inserted 586389 rows...\n",
      "      -> Inserted 595305 rows...\n",
      "      -> Inserted 604197 rows...\n",
      "      -> Inserted 613131 rows...\n",
      "      -> Inserted 622105 rows...\n",
      "      -> Inserted 631119 rows...\n",
      "      -> Inserted 640053 rows...\n",
      "      -> Inserted 649091 rows...\n",
      "      -> Inserted 658142 rows...\n",
      "      -> Inserted 667264 rows...\n",
      "      -> Inserted 676353 rows...\n",
      "      -> Inserted 685472 rows...\n",
      "      -> Inserted 694636 rows...\n",
      "      -> Inserted 699243 rows...\n",
      "      -> Inserted 703450 rows...\n",
      "      -> Inserted 707901 rows...\n",
      "      -> Inserted 712328 rows...\n",
      "      -> Inserted 716922 rows...\n",
      "      -> Inserted 721613 rows...\n",
      "      -> Inserted 726369 rows...\n",
      "      -> Inserted 731213 rows...\n",
      "      -> Inserted 736159 rows...\n",
      "      -> Inserted 740715 rows...\n",
      "      -> Inserted 743835 rows...\n",
      "      -> Inserted 746983 rows...\n",
      "      -> Inserted 750134 rows...\n",
      "      -> Inserted 753283 rows...\n",
      "      -> Inserted 756514 rows...\n",
      "      -> Inserted 759752 rows...\n",
      "      -> Inserted 763068 rows...\n",
      "      -> Inserted 766542 rows...\n",
      "      -> Inserted 770026 rows...\n",
      "      -> Inserted 773525 rows...\n",
      "      -> Inserted 777172 rows...\n",
      "      -> Inserted 780763 rows...\n",
      "      -> Inserted 784385 rows...\n",
      "      -> Inserted 788097 rows...\n",
      "      -> Inserted 791887 rows...\n",
      "      -> Inserted 795854 rows...\n",
      "      -> Inserted 799870 rows...\n",
      "      -> Inserted 803858 rows...\n",
      "      -> Inserted 807988 rows...\n",
      "      -> Inserted 812061 rows...\n",
      "      -> Inserted 816185 rows...\n",
      "      -> Inserted 820462 rows...\n",
      "      -> Inserted 827221 rows...\n",
      "      -> Inserted 837212 rows...\n",
      "      -> Inserted 847211 rows...\n",
      "      -> Inserted 857210 rows...\n",
      "      -> Inserted 867207 rows...\n",
      "      -> Inserted 877207 rows...\n",
      "      -> Inserted 887205 rows...\n",
      "      -> Inserted 897205 rows...\n",
      "      -> Inserted 907205 rows...\n",
      "      -> Inserted 917205 rows...\n",
      "      -> Inserted 927205 rows...\n",
      "      -> Inserted 937205 rows...\n",
      "      -> Inserted 947205 rows...\n",
      "      -> Inserted 957205 rows...\n",
      "      -> Inserted 967205 rows...\n",
      "      -> Inserted 977205 rows...\n",
      "      -> Inserted 987205 rows...\n",
      "      -> Inserted 997205 rows...\n",
      "      -> Inserted 1007205 rows...\n",
      "      -> Inserted 1016653 rows...\n",
      "      -> Inserted 1025714 rows...\n",
      "      -> Inserted 1034842 rows...\n",
      "      -> Inserted 1044009 rows...\n",
      "      -> Inserted 1053286 rows...\n",
      "      -> Inserted 1062517 rows...\n",
      "      -> Inserted 1071707 rows...\n",
      "      -> Inserted 1080961 rows...\n",
      "      -> Inserted 1090314 rows...\n",
      "      -> Inserted 1099689 rows...\n",
      "      -> Inserted 1109094 rows...\n",
      "      -> Inserted 1118475 rows...\n",
      "      -> Inserted 1127842 rows...\n",
      "      -> Inserted 1137141 rows...\n",
      "      -> Inserted 1146525 rows...\n",
      "      -> Inserted 1155934 rows...\n",
      "      -> Inserted 1165366 rows...\n",
      "      -> Inserted 1174854 rows...\n",
      "      -> Inserted 1184346 rows...\n",
      "      -> Inserted 1193852 rows...\n",
      "      -> Inserted 1203375 rows...\n",
      "      -> Inserted 1213127 rows...\n",
      "      -> Inserted 1223062 rows...\n",
      "      -> Inserted 1233044 rows...\n",
      "      -> Inserted 1243044 rows...\n",
      "      -> Inserted 1253044 rows...\n",
      "      -> Inserted 1263044 rows...\n",
      "      -> Inserted 1272315 rows...\n",
      "      -> Inserted 1274170 rows...\n",
      "      -> Inserted 1276443 rows...\n",
      "      -> Inserted 1278906 rows...\n",
      "      -> Inserted 1281395 rows...\n",
      "      -> Inserted 1283960 rows...\n",
      "      -> Inserted 1286520 rows...\n",
      "      -> Inserted 1289167 rows...\n",
      "      -> Inserted 1291906 rows...\n",
      "      -> Inserted 1294690 rows...\n",
      "      -> Inserted 1297552 rows...\n",
      "      -> Inserted 1300430 rows...\n",
      "      -> Inserted 1303393 rows...\n",
      "      -> Inserted 1303607 rows...\n",
      "‚úÖ ETL SUCCESS: MariaDB table 'loans_clean' is fully populated.\n",
      "\n",
      "üïµÔ∏è [Phase 3] Validating Final SQL Data...\n",
      "üéâ FINAL SUCCESS: Pipeline Verified! Data is ready for Feature Engineering.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, Table, Column, Integer, String, Float, DateTime, MetaData\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import math\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "print(\"Starting Master ETL Pipeline...\")\n",
    "\n",
    "DB_USER = 'admin'\n",
    "DB_PASS = 'Admin@1234Strong!'  \n",
    "DB_HOST = '127.0.0.1'\n",
    "DB_PORT = '3306'\n",
    "TARGET_DB = 'lending_club'\n",
    "\n",
    "CSV_PATH = \"/home/nanak/mlops/data/raw/loan.csv\"\n",
    "\n",
    "DB_PASS_ENCODED = quote_plus(DB_PASS)\n",
    "\n",
    "\n",
    "def safe_float(val, default=0.0):\n",
    "    try:\n",
    "        if pd.isna(val) or val is None: return default\n",
    "        f = float(val)\n",
    "        return default if math.isnan(f) else f\n",
    "    except: return default\n",
    "\n",
    "def parse_percent(val):\n",
    "    try:\n",
    "        if isinstance(val, str): return safe_float(val.strip('%'))\n",
    "        return safe_float(val)\n",
    "    except: return 0.0\n",
    "\n",
    "def parse_term(val):\n",
    "    try: return safe_float(str(val).strip().split()[0])\n",
    "    except: return None\n",
    "\n",
    "def parse_date(val):\n",
    "    try: return datetime.strptime(str(val), '%b-%Y')\n",
    "    except: return None\n",
    "\n",
    "def map_emp(val):\n",
    "    m = {'< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4,\n",
    "         '5 years': 5, '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9, '10+ years': 10}\n",
    "    return m.get(val, 0)\n",
    "\n",
    "def calculate_fico_score(low, high):\n",
    "    \"\"\"Calculates average FICO and ensures it is within logical bounds.\"\"\"\n",
    "    if low is None or high is None:\n",
    "        return 600.0 # Default fallback\n",
    "    score = (safe_float(low) + safe_float(high)) / 2\n",
    "    return max(300, min(850, score))\n",
    "\n",
    "\n",
    "print(\"\\n Establishing Database Connection...\")\n",
    "\n",
    "server_url = f\"mysql+pymysql://{DB_USER}:{DB_PASS_ENCODED}@{DB_HOST}:{DB_PORT}/\"\n",
    "engine_server = create_engine(server_url)\n",
    "\n",
    "try:\n",
    "    with engine_server.connect() as conn:\n",
    "        conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "        conn.execute(text(f\"CREATE DATABASE IF NOT EXISTS {TARGET_DB};\"))\n",
    "    print(f\"Database '{TARGET_DB}' verified/created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection Error: {e}\")\n",
    "    raise e\n",
    "\n",
    "engine = create_engine(f\"{server_url}{TARGET_DB}\")\n",
    "metadata = MetaData()\n",
    "\n",
    "loans_clean = Table(\n",
    "    'loans_clean', metadata,\n",
    "    Column('target', Integer),\n",
    "    Column('loan_amnt', Float),\n",
    "    Column('int_rate', Float),\n",
    "    Column('term', Float),\n",
    "    Column('grade', String(5)),\n",
    "    Column('sub_grade', String(5)), \n",
    "    Column('purpose', String(100)),\n",
    "    Column('application_type', String(50)), \n",
    "    Column('annual_inc', Float),\n",
    "    Column('dti', Float),\n",
    "    Column('emp_length_num', Float),\n",
    "    Column('home_ownership', String(50)),\n",
    "    Column('verification_status', Integer),\n",
    "    Column('fico_score', Float),\n",
    "    Column('revol_util', Float),\n",
    "    Column('open_acc', Float),\n",
    "    Column('pub_rec', Float),\n",
    "    Column('addr_state', String(5)),\n",
    "    Column('issue_date', DateTime),\n",
    "    Column('issue_year', Integer)\n",
    ")\n",
    "\n",
    "metadata.drop_all(engine)\n",
    "metadata.create_all(engine)\n",
    "\n",
    "\n",
    "print(\"\\n Running ETL (Extract -> Transform -> Load)...\")\n",
    "\n",
    "chunk_size = 10000\n",
    "total_rows = 0\n",
    "\n",
    "try:\n",
    "    print(f\"   -> Processing CSV Stream: {CSV_PATH}\")g\n",
    "    for chunk in pd.read_csv(CSV_PATH, chunksize=chunk_size, low_memory=False):\n",
    "        \n",
    "        chunk = chunk[chunk['loan_status'].isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "        if chunk.empty: continue\n",
    "        \n",
    "        chunk.columns = [c.replace(' ', '_').lower() for c in chunk.columns]\n",
    "        \n",
    "        batch_data = []\n",
    "        for _, row in chunk.iterrows():\n",
    "            dt = parse_date(row.get('issue_d'))\n",
    "            \n",
    "            clean_row = {\n",
    "                'target': 1 if row['loan_status'] == 'Charged Off' else 0,\n",
    "                'loan_amnt': safe_float(row.get('loan_amnt')),\n",
    "                'int_rate': safe_float(row.get('int_rate')),\n",
    "                'term': parse_term(row.get('term')),\n",
    "                'grade': str(row.get('grade')),\n",
    "                'sub_grade': str(row.get('sub_grade')),\n",
    "                'purpose': str(row.get('purpose')),\n",
    "                'application_type': str(row.get('application_type')),\n",
    "                'annual_inc': safe_float(row.get('annual_inc')),\n",
    "                'dti': safe_float(row.get('dti')),\n",
    "                'emp_length_num': map_emp(row.get('emp_length')),\n",
    "                'home_ownership': str(row.get('home_ownership')),\n",
    "                'verification_status': 0 if row.get('verification_status') == 'Not Verified' else 1,\n",
    "                'fico_score': calculate_fico_score(row.get('fico_range_low'), row.get('fico_range_high')),\n",
    "                'revol_util': parse_percent(row.get('revol_util')),\n",
    "                'open_acc': safe_float(row.get('open_acc')),\n",
    "                'pub_rec': safe_float(row.get('pub_rec')),\n",
    "                'addr_state': str(row.get('addr_state')),\n",
    "                'issue_date': dt,\n",
    "                'issue_year': dt.year if dt else None\n",
    "            }\n",
    "            batch_data.append(clean_row)\n",
    "\n",
    "        if batch_data:\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(loans_clean.insert(), batch_data)\n",
    "            total_rows += len(batch_data)\n",
    "            print(f\"      -> Inserted {total_rows} rows...\")\n",
    "\n",
    "    print(\"ETL SUCCESS: MariaDB table 'loans_clean' is fully populated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ETL Failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "print(\"Validating Final SQL Data...\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # 1. Target check\n",
    "        target_check = conn.execute(text(\"SELECT COUNT(*) FROM loans_clean WHERE target NOT IN (0, 1)\")).scalar()\n",
    "        # 2. FICO check\n",
    "        fico_check = conn.execute(text(\"SELECT COUNT(*) FROM loans_clean WHERE fico_score < 300 OR fico_score > 850\")).scalar()\n",
    "        # 3. Income check\n",
    "        income_check = conn.execute(text(\"SELECT COUNT(*) FROM loans_clean WHERE annual_inc IS NULL\")).scalar()\n",
    "\n",
    "    if target_check == 0 and fico_check == 0 and income_check == 0:\n",
    "        print(\"FINAL SUCCESS: Pipeline Verified! Data is ready for Feature Engineering.\")\n",
    "    else:\n",
    "        print(f\"WARNING: Data issues found! Targets: {target_check}, FICO errors: {fico_check}, NULL Incomes: {income_check}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error during Post-Validation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db9814-d8e5-4a9d-8fa8-b1c3e70a877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7e8ed3-31dc-4293-a4f9-1a4821d19302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sqlalchemy import create_engine, text, Table, Column, Integer, String, Float, DateTime, MetaData\n",
    "# from datetime import datetime\n",
    "# import warnings\n",
    "# import math\n",
    "# from urllib.parse import quote_plus\n",
    "# import csv\n",
    "\n",
    "# # 1. Hide warnings\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# print(\"üöÄ Starting Advanced Model-Ready ETL Pipeline...\")\n",
    "\n",
    "# # ==========================================\n",
    "# # PART 1: CONFIGURATION\n",
    "# # ==========================================\n",
    "# DB_USER, DB_PASS = 'admin', 'Admin@1234Strong!'\n",
    "# DB_HOST, DB_PORT = '127.0.0.1', '3306'\n",
    "# TARGET_DB = 'lending_club'\n",
    "# CSV_PATH = \"/home/nanak/mlops/data/raw/loan.csv\"\n",
    "\n",
    "# DB_PASS_ENCODED = quote_plus(DB_PASS)\n",
    "\n",
    "# # --- Enhanced Helper Functions ---\n",
    "# def safe_float(val, default=0.0):\n",
    "#     try:\n",
    "#         if val is None or str(val).strip() == \"\" or str(val).lower() == 'nan': \n",
    "#             return default\n",
    "#         return float(str(val).replace('%', '').strip())\n",
    "#     except: return default\n",
    "\n",
    "# def map_emp_length(val):\n",
    "#     m = {'< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4,\n",
    "#          '5 years': 5, '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9, '10+ years': 10}\n",
    "#     return float(m.get(val, 0))\n",
    "\n",
    "# # ==========================================\n",
    "# # PART 2: DATABASE INITIALIZATION\n",
    "# # ==========================================\n",
    "# server_url = f\"mysql+pymysql://{DB_USER}:{DB_PASS_ENCODED}@{DB_HOST}:{DB_PORT}/\"\n",
    "# engine = create_engine(f\"{server_url}{TARGET_DB}\")\n",
    "# metadata = MetaData()\n",
    "\n",
    "# # Define the \"High Quality\" Table Schema\n",
    "# loans_clean = Table(\n",
    "#     'loans_clean', metadata,\n",
    "#     Column('target', Integer),\n",
    "#     Column('loan_amnt', Float),\n",
    "#     Column('int_rate', Float),\n",
    "#     Column('annual_inc', Float),\n",
    "#     Column('dti', Float),\n",
    "#     Column('fico_score', Float), # We will use fallback logic for this\n",
    "#     Column('term_months', Integer),\n",
    "#     Column('emp_length_num', Float),\n",
    "#     Column('delinq_2yrs', Float),\n",
    "#     Column('mths_since_last_delinq', Float),\n",
    "#     Column('tot_cur_bal', Float),\n",
    "#     Column('revol_util', Float),\n",
    "#     Column('num_tl_op_past_12m', Float),\n",
    "#     Column('tot_coll_amt', Float),\n",
    "#     Column('is_verified', Integer),\n",
    "#     Column('grade', String(5)),\n",
    "#     Column('home_ownership', String(20)),\n",
    "#     Column('purpose', String(50))\n",
    "# )\n",
    "\n",
    "# print(\"‚öôÔ∏è Dropping and recreating table...\")\n",
    "# metadata.drop_all(engine)\n",
    "# metadata.create_all(engine)\n",
    "\n",
    "# # ==========================================\n",
    "# # PART 3: EXTRACTION & TRANSFORMATION (Streaming)\n",
    "# # ==========================================\n",
    "# print(\"\\nüîÑ Starting Stream Transformation...\")\n",
    "\n",
    "# total_rows = 0\n",
    "# null_stats = {\"fico\": 0, \"income\": 0, \"dti\": 0}\n",
    "\n",
    "# with open(CSV_PATH, 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "    \n",
    "#     # Verification: Check if FICO columns actually exist in the header\n",
    "#     headers = reader.fieldnames\n",
    "#     has_fico = 'fico_range_low' in headers\n",
    "    \n",
    "#     batch = []\n",
    "#     for row in reader:\n",
    "#         # 1. FILTER: Target Definition\n",
    "#         if row['loan_status'] not in ['Fully Paid', 'Charged Off']:\n",
    "#             continue\n",
    "\n",
    "#         # 2. TRANSFORM: FICO Score Logic (The \"Problem Solver\")\n",
    "#         # If columns missing or 0, default to 680 (Neutral)\n",
    "#         if has_fico:\n",
    "#             f_low = safe_float(row.get('fico_range_low'))\n",
    "#             f_high = safe_float(row.get('fico_range_high'))\n",
    "#             fico_val = (f_low + f_high) / 2\n",
    "#         else:\n",
    "#             fico_val = 0\n",
    "            \n",
    "#         if fico_val < 300: # Invalid or Missing\n",
    "#             fico_val = 680.0\n",
    "#             null_stats[\"fico\"] += 1\n",
    "\n",
    "#         # 3. TRANSFORM: Income Imputation\n",
    "#         income = safe_float(row.get('annual_inc'))\n",
    "#         if income <= 0:\n",
    "#             income = 50000.0 # Standard median fallback\n",
    "#             null_stats[\"income\"] += 1\n",
    "\n",
    "#         # 4. TRANSFORM: Term Parsing\n",
    "#         term_str = row.get('term', '36')\n",
    "#         term_val = 36 if '36' in term_str else 60\n",
    "\n",
    "#         # 5. MAPPING: Categoricals and Null-Heavy Columns\n",
    "#         clean_row = {\n",
    "#             'target': 1 if row['loan_status'] == 'Charged Off' else 0,\n",
    "#             'loan_amnt': safe_float(row.get('loan_amnt')),\n",
    "#             'int_rate': safe_float(row.get('int_rate')),\n",
    "#             'annual_inc': income,\n",
    "#             'dti': safe_float(row.get('dti')),\n",
    "#             'fico_score': fico_val,\n",
    "#             'term_months': term_val,\n",
    "#             'emp_length_num': map_emp_length(row.get('emp_length')),\n",
    "#             'delinq_2yrs': safe_float(row.get('delinq_2yrs')),\n",
    "#             # Logical Null: If missing, they probably never had a delinquency. Set to 999 months.\n",
    "#             'mths_since_last_delinq': safe_float(row.get('mths_since_last_delinq'), default=999.0),\n",
    "#             'tot_cur_bal': safe_float(row.get('tot_cur_bal')),\n",
    "#             'revol_util': safe_float(row.get('revol_util')),\n",
    "#             'num_tl_op_past_12m': safe_float(row.get('num_tl_op_past_12m')),\n",
    "#             'tot_coll_amt': safe_float(row.get('tot_coll_amt')),\n",
    "#             'is_verified': 1 if row.get('verification_status') != 'Not Verified' else 0,\n",
    "#             'grade': row.get('grade', 'U'),\n",
    "#             'home_ownership': row.get('home_ownership', 'OTHER'),\n",
    "#             'purpose': row.get('purpose', 'other')\n",
    "#         }\n",
    "        \n",
    "#         batch.append(clean_row)\n",
    "        \n",
    "#         # 6. LOAD: Batching for Speed\n",
    "#         if len(batch) >= 10000:\n",
    "#             with engine.begin() as conn:\n",
    "#                 conn.execute(loans_clean.insert(), batch)\n",
    "#             total_rows += len(batch)\n",
    "#             print(f\"   -> Processed {total_rows} rows...\")\n",
    "#             batch = []\n",
    "\n",
    "#     # Final batch\n",
    "#     if batch:\n",
    "#         with engine.begin() as conn:\n",
    "#             conn.execute(loans_clean.insert(), batch)\n",
    "#         total_rows += len(batch)\n",
    "\n",
    "# print(f\"\\n‚úÖ SUCCESS: Integrated {total_rows} rows.\")\n",
    "# print(f\"üìä Quality Note: Imputed {null_stats['fico']} missing FICO scores and {null_stats['income']} missing incomes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0d63ef-a501-42d8-885f-11c3efd102b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing MariaDB Schema (Excluding FICO)...\n",
      "‚öôÔ∏è Streaming Data to DB...\n",
      "‚úÖ Ingestion Complete.\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# import math\n",
    "# import warnings\n",
    "# from datetime import datetime\n",
    "# from sqlalchemy import create_engine, text, Table, Column, Integer, String, Float, MetaData\n",
    "# from urllib.parse import quote_plus\n",
    "\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# # 1. Configuration\n",
    "# DB_USER, DB_PASS = 'admin', 'Admin@1234Strong!'\n",
    "# DB_HOST, DB_PORT = '127.0.0.1', '3306'\n",
    "# TARGET_DB = 'lending_club'\n",
    "# CSV_PATH = \"/home/nanak/mlops/data/raw/loan.csv\"\n",
    "\n",
    "# # Connection\n",
    "# PASS_ENC = quote_plus(DB_PASS)\n",
    "# BASE_URL = f\"mysql+pymysql://{DB_USER}:{PASS_ENC}@{DB_HOST}:{DB_PORT}/\"\n",
    "# engine = create_engine(f\"{BASE_URL}{TARGET_DB}\")\n",
    "\n",
    "# # Helpers\n",
    "# def safe_float(val, default=0.0):\n",
    "#     try: return float(str(val).replace('%', '').strip()) if val else default\n",
    "#     except: return default\n",
    "\n",
    "# # ==========================================\n",
    "# # 1. SCHEMA SETUP\n",
    "# # ==========================================\n",
    "# print(\"üöÄ Initializing MariaDB Schema (Excluding FICO)...\")\n",
    "# root_engine = create_engine(BASE_URL)\n",
    "# with root_engine.connect() as conn:\n",
    "#     conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "#     conn.execute(text(f\"CREATE DATABASE IF NOT EXISTS {TARGET_DB}\"))\n",
    "\n",
    "# metadata = MetaData()\n",
    "# loans_clean = Table(\n",
    "#     'loans_clean', metadata,\n",
    "#     Column('target', Integer),\n",
    "#     Column('loan_amnt', Float),\n",
    "#     Column('int_rate', Float),\n",
    "#     Column('annual_inc', Float),\n",
    "#     Column('dti', Float),\n",
    "#     Column('revol_util', Float),\n",
    "#     Column('delinq_2yrs', Float),\n",
    "#     Column('grade', String(10)),\n",
    "#     Column('home_ownership', String(20)),\n",
    "#     Column('term', String(20))\n",
    "# )\n",
    "# metadata.drop_all(engine)\n",
    "# metadata.create_all(engine)\n",
    "\n",
    "# # ==========================================\n",
    "# # 2. STREAMING INGESTION\n",
    "# # ==========================================\n",
    "# print(\"‚öôÔ∏è Streaming Data to DB...\")\n",
    "# batch = []\n",
    "# with open(CSV_PATH, 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     for row in reader:\n",
    "#         if row['loan_status'] not in ['Fully Paid', 'Charged Off']:\n",
    "#             continue\n",
    "        \n",
    "#         batch.append({\n",
    "#             'target': 1 if row['loan_status'] == 'Charged Off' else 0,\n",
    "#             'loan_amnt': safe_float(row.get('loan_amnt')),\n",
    "#             'int_rate': safe_float(row.get('int_rate')),\n",
    "#             'annual_inc': safe_float(row.get('annual_inc'), default=50000.0), # Impute median\n",
    "#             'dti': safe_float(row.get('dti')),\n",
    "#             'revol_util': safe_float(row.get('revol_util')),\n",
    "#             'delinq_2yrs': safe_float(row.get('delinq_2yrs')),\n",
    "#             'grade': row.get('grade', 'U'),\n",
    "#             'home_ownership': row.get('home_ownership', 'OTHER'),\n",
    "#             'term': row.get('term', '36 months')\n",
    "#         })\n",
    "        \n",
    "#         if len(batch) >= 10000:\n",
    "#             with engine.begin() as conn:\n",
    "#                 conn.execute(loans_clean.insert(), batch)\n",
    "#             batch = []\n",
    "\n",
    "# if batch:\n",
    "#     with engine.begin() as conn:\n",
    "#         conn.execute(loans_clean.insert(), batch)\n",
    "\n",
    "# print(\"‚úÖ Ingestion Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d741c3-63db-42ba-8772-06b980ec2b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column: {'loan_amnt_null': Decimal('0'), 'int_rate_null': Decimal('0'), 'annual_inc_null': Decimal('0'), 'dti_null': Decimal('0'), 'revol_util_null': Decimal('0'), 'delinq_2yrs_null': Decimal('0'), 'grade_null': Decimal('0'), 'home_ownership_null': Decimal('0'), 'term_null': Decimal('0')}\n"
     ]
    }
   ],
   "source": [
    "# columns_to_check = [\n",
    "#     'loan_amnt', 'int_rate', 'annual_inc', 'dti', 'revol_util', 'delinq_2yrs', \n",
    "#     'grade', 'home_ownership', 'term'\n",
    "# ]\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     query = \"SELECT \" + \", \".join(\n",
    "#         [f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS {col}_null\" for col in columns_to_check]\n",
    "#     ) + \" FROM loans_clean\"\n",
    "    \n",
    "#     result = conn.execute(text(query)).fetchone()\n",
    "#     print(\"Missing values per column:\", dict(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50abf6e-d1bf-4a08-906e-43e354c9f5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric ranges: {'min_loan_amnt': 500.0, 'max_loan_amnt': 40000.0, 'min_int_rate': 5.31, 'max_int_rate': 30.99, 'min_annual_inc': 0.0, 'max_annual_inc': 10999200.0, 'min_dti': -1.0, 'max_dti': 999.0, 'min_revol_util': 0.0, 'max_revol_util': 892.3, 'min_delinq_2yrs': 0.0, 'max_delinq_2yrs': 39.0}\n"
     ]
    }
   ],
   "source": [
    "# numeric_cols = ['loan_amnt','int_rate','annual_inc','dti','revol_util','delinq_2yrs']\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     query = \"SELECT \" + \", \".join(\n",
    "#         [f\"MIN({col}) AS min_{col}, MAX({col}) AS max_{col}\" for col in numeric_cols]\n",
    "#     ) + \" FROM loans_clean\"\n",
    "    \n",
    "#     result = conn.execute(text(query)).fetchone()\n",
    "#     print(\"Numeric ranges:\", dict(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a94415-b7ca-46b4-a4b5-da7f79db771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in grade:\n",
      "  A: 226243\n",
      "  B: 380152\n",
      "  C: 369928\n",
      "  D: 195280\n",
      "  E: 91570\n",
      "  F: 31483\n",
      "  G: 8951\n",
      "\n",
      "Unique values in home_ownership:\n",
      "  ANY: 267\n",
      "  MORTGAGE: 645496\n",
      "  NONE: 48\n",
      "  OTHER: 144\n",
      "  OWN: 139844\n",
      "  RENT: 517808\n",
      "\n",
      "Unique values in term:\n",
      "   36 months: 988754\n",
      "   60 months: 314853\n"
     ]
    }
   ],
   "source": [
    "# categorical_cols = ['grade','home_ownership','term']\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     for col in categorical_cols:\n",
    "#         print(f\"\\nUnique values in {col}:\")\n",
    "#         rows = conn.execute(text(f\"SELECT {col}, COUNT(*) FROM loans_clean GROUP BY {col}\")).fetchall()\n",
    "#         for val, cnt in rows:\n",
    "#             print(f\"  {val}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccaa8df-7cb1-46ed-99e7-c5fd76df0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "  0: 1041952\n",
      "  1: 261655\n"
     ]
    }
   ],
   "source": [
    "# with engine.connect() as conn:\n",
    "#     rows = conn.execute(text(\"SELECT target, COUNT(*) FROM loans_clean GROUP BY target\")).fetchall()\n",
    "#     print(\"Target distribution:\")\n",
    "#     for val, cnt in rows:\n",
    "#         print(f\"  {val}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dec86c5c-a844-4581-b5a0-a30c9b062e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Step 1: Ingesting Raw Data with FICO Fix...\n",
      "‚úÖ Ingestion with real FICO check complete.\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "\n",
    "# def run_feature_ingestion():\n",
    "#     print(\"üöÄ Step 1: Ingesting Raw Data with FICO Fix...\")\n",
    "#     # SQL Schema Update (Removed FICO fallback, strictly using raw data)\n",
    "#     query_create = \"\"\"\n",
    "#     CREATE OR REPLACE TABLE loans_clean (\n",
    "#         target INT, loan_amnt FLOAT, int_rate FLOAT, annual_inc FLOAT, dti FLOAT,\n",
    "#         revol_util FLOAT, delinq_2yrs FLOAT, fico_score FLOAT,\n",
    "#         grade VARCHAR(5), home_ownership VARCHAR(20), term VARCHAR(20)\n",
    "#     );\n",
    "#     \"\"\"\n",
    "#     with engine.connect() as conn:\n",
    "#         conn.execution_options(isolation_level=\"AUTOCOMMIT\").execute(text(query_create))\n",
    "\n",
    "#     batch = []\n",
    "#     with open(CSV_PATH, 'r', encoding='utf-8') as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         for row in reader:\n",
    "#             if row['loan_status'] not in ['Fully Paid', 'Charged Off']: continue\n",
    "            \n",
    "#             # --- PROBLEM SOLVING: FICO FIX ---\n",
    "#             # Try original columns first, then 'last' columns\n",
    "#             f_l = safe_float(row.get('fico_range_low')) or safe_float(row.get('last_fico_range_low'))\n",
    "#             f_h = safe_float(row.get('fico_range_high')) or safe_float(row.get('last_fico_range_high'))\n",
    "#             fico = (f_l + f_h) / 2 if (f_l and f_h) else None # Keep as None if missing\n",
    "\n",
    "#             batch.append({\n",
    "#                 'target': 1 if row['loan_status'] == 'Charged Off' else 0,\n",
    "#                 'loan_amnt': safe_float(row.get('loan_amnt')),\n",
    "#                 'int_rate': safe_float(row.get('int_rate')),\n",
    "#                 'annual_inc': safe_float(row.get('annual_inc')),\n",
    "#                 'dti': safe_float(row.get('dti')),\n",
    "#                 'revol_util': safe_float(row.get('revol_util')),\n",
    "#                 'delinq_2yrs': safe_float(row.get('delinq_2yrs')),\n",
    "#                 'fico_score': fico, # SQL will handle NULL\n",
    "#                 'grade': row.get('grade', 'U'),\n",
    "#                 'home_ownership': row.get('home_ownership', 'OTHER'),\n",
    "#                 'term': row.get('term', '36 months')\n",
    "#             })\n",
    "#             if len(batch) >= 10000:\n",
    "#                 with engine.begin() as conn: conn.execute(loans_clean.insert(), batch)\n",
    "#                 batch = []\n",
    "#     if batch:\n",
    "#         with engine.begin() as conn: conn.execute(loans_clean.insert(), batch)\n",
    "#     print(\"‚úÖ Ingestion with real FICO check complete.\")\n",
    "\n",
    "# run_feature_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82920c1a-ef51-452d-aa7e-c1243250d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_feature_engineering():\n",
    "#     print(\"üöÄ Step 2: Creating Behavioral Features...\")\n",
    "#     query = \"\"\"\n",
    "#     CREATE OR REPLACE TABLE loans_features AS\n",
    "#     SELECT \n",
    "#         target,\n",
    "#         -- 1. AFFORDABILITY Interaction\n",
    "#         (loan_amnt / (annual_inc + 1)) AS payment_burden,\n",
    "        \n",
    "#         -- 2. CREDIT STRESS Interaction\n",
    "#         (revol_util * dti) / 100.0 AS credit_stress_index,\n",
    "\n",
    "#         -- 3. RAW SIGNALS\n",
    "#         int_rate, dti, revol_util, annual_inc,\n",
    "#         CASE WHEN home_ownership = 'RENT' THEN 1 ELSE 0 END AS is_rent,\n",
    "#         CASE WHEN term LIKE '%60%' THEN 1 ELSE 0 END AS is_long_term\n",
    "#     FROM loans_clean;\n",
    "#     \"\"\"\n",
    "#     with engine.connect() as conn:\n",
    "#         conn.execution_options(isolation_level=\"AUTOCOMMIT\").execute(text(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a693b2dc-89fb-4397-aef5-b71e48750c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Step 3: Scaling into Golden Matrix...\n",
      "‚úÖ Final scaling complete.\n"
     ]
    }
   ],
   "source": [
    "# def run_preprocessing():\n",
    "#     print(\"‚öñÔ∏è Step 3: Scaling into Golden Matrix...\")\n",
    "#     query = \"\"\"\n",
    "#     CREATE OR REPLACE TABLE loans_processed AS\n",
    "#     SELECT \n",
    "#         target, is_rent, is_long_term,\n",
    "#         (fico_score - 300) / 550.0 AS fico_scaled,\n",
    "#         (int_rate / 31.0) AS rate_scaled,\n",
    "#         (dti / 100.0) AS dti_scaled,\n",
    "#         (LOG(1 + annual_inc) / 17.0) AS income_scaled,\n",
    "#         (payment_burden / 1.0) AS burden_scaled,\n",
    "#         (risk_interaction / 50.0) AS risk_scaled\n",
    "#     FROM loans_features;\n",
    "#     \"\"\"\n",
    "#     with engine.connect() as conn:\n",
    "#         conn.execution_options(isolation_level=\"AUTOCOMMIT\").execute(text(query))\n",
    "#     print(\"‚úÖ Final scaling complete.\")\n",
    "\n",
    "# run_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00fdc780-989a-492d-82ad-c26e9e0f2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Step 3: Training with AUC-PR and Optimized Threshold...\n",
      "üéØ Best F1 Threshold found: 0.5214\n",
      "\n",
      "üìù PERFORMANCE REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.66      0.75    208391\n",
      "         1.0       0.31      0.63      0.42     52331\n",
      "\n",
      "    accuracy                           0.65    260722\n",
      "   macro avg       0.60      0.64      0.58    260722\n",
      "weighted avg       0.76      0.65      0.68    260722\n",
      "\n",
      "\n",
      "üìâ CONFUSION MATRIX:\n",
      "[[136578  71813]\n",
      " [ 19386  32945]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "\n",
    "# def train_and_evaluate():\n",
    "#     print(\"\\nüèãÔ∏è Step 3: Training with AUC-PR and Optimized Threshold...\")\n",
    "    \n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(text(\"SELECT * FROM loans_processed\"))\n",
    "#         data = np.array(result.fetchall(), dtype=np.float32)\n",
    "\n",
    "#     y, X = data[:, 0], data[:, 1:]\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#     X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "\n",
    "#     # 1. Train specifically for Precision-Recall Area (aucpr)\n",
    "#     model = xgb.XGBClassifier(\n",
    "        \n",
    "#         n_estimators=1000,\n",
    "#         max_depth=3,              # Slightly shallower to prevent overfitting\n",
    "#         learning_rate=0.05,\n",
    "#         scale_pos_weight=4.0,     # Matches your 20/80 imbalance perfectly\n",
    "#         colsample_bytree=0.2,     # Force the model to look at DTI and Income\n",
    "#         subsample=0.8,            # Use 80% of data per tree\n",
    "#         eval_metric='aucpr',\n",
    "#         early_stopping_rounds=25\n",
    "#     )\n",
    "    \n",
    "#     model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "#     # 2. Probability Generation\n",
    "#     y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#     # 3. Finding the \"Mathematical Sweet Spot\" for Threshold\n",
    "#     precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "#     # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     f1_scores = (2 * precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "#     best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "    \n",
    "#     # 4. Final Prediction with Optimized Threshold\n",
    "#     y_pred = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "#     print(f\"üéØ Best F1 Threshold found: {best_thresh:.4f}\")\n",
    "#     print(\"\\nüìù PERFORMANCE REPORT:\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n",
    "#     print(\"\\nüìâ CONFUSION MATRIX:\")\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b36850c8-cad4-41af-8512-770c09a54e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Step 5: Balancing Precision and Recall...\n",
      "üéØ Mathematical Best Threshold: 0.5014\n",
      "\n",
      "üìù BALANCED PERFORMANCE REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.61      0.72    208391\n",
      "         1.0       0.30      0.67      0.42     52331\n",
      "\n",
      "    accuracy                           0.63    260722\n",
      "   macro avg       0.59      0.64      0.57    260722\n",
      "weighted avg       0.76      0.63      0.66    260722\n",
      "\n",
      "\n",
      "üìâ BALANCED CONFUSION MATRIX:\n",
      "[[128086  80305]\n",
      " [ 17410  34921]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, f1_score\n",
    "\n",
    "# def run_final_balanced_training():\n",
    "#     print(\"\\nüèãÔ∏è Step 5: Balancing Precision and Recall...\")\n",
    "    \n",
    "#     # 1. Fetch data\n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(text(\"SELECT * FROM loans_processed\"))\n",
    "#         data = np.array(result.fetchall(), dtype=np.float32)\n",
    "\n",
    "#     y = data[:, 0]\n",
    "#     X = data[:, 1:]\n",
    "    \n",
    "#     # 2. Drop 0-importance columns (term and fico) based on previous audit\n",
    "#     X_cleaned = np.delete(X, [1, 2], axis=1) \n",
    "\n",
    "#     # 3. Split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#     X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "\n",
    "#     # 4. Train\n",
    "#     model = xgb.XGBClassifier(\n",
    "#         n_estimators=1000,\n",
    "#         max_depth=5,\n",
    "#         learning_rate=0.05,\n",
    "#         scale_pos_weight=4, \n",
    "#         colsample_bytree=0.5,\n",
    "#         eval_metric='aucpr',\n",
    "#         early_stopping_rounds=20\n",
    "#     )\n",
    "    \n",
    "#     model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "#     # --- THE FIX: Define y_probs clearly ---\n",
    "#     # Probability for the 'Default' class (1)\n",
    "#     y_probs = model.predict_proba(X_test)[:, 1] \n",
    "\n",
    "#     # 5. Threshold Optimization\n",
    "#     precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "#     f1_scores = (2 * precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "#     best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "    \n",
    "#     # Apply\n",
    "#     y_pred = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "#     print(f\"üéØ Mathematical Best Threshold: {best_thresh:.4f}\")\n",
    "#     print(\"\\nüìù BALANCED PERFORMANCE REPORT:\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n",
    "#     print(\"\\nüìâ BALANCED CONFUSION MATRIX:\")\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "#     # Return everything we need for the next cells\n",
    "#     return model, X_test, y_test, y_probs\n",
    "\n",
    "# # Execute the function\n",
    "# model, X_test, y_test, y_probs = run_final_balanced_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e6d9b16-d298-41b1-b642-b542d1673c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: y_probs has 260722 values.\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Check: y_probs has {len(y_probs)} values.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
