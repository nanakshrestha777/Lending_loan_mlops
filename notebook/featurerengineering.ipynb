{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e190176-5d2e-4321-a612-81eaf37b446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sqlalchemy import create_engine, text, exc as sa_exc\n",
    "from urllib.parse import quote_plus\n",
    "import warnings\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"preprocessing\")\n",
    "warnings.filterwarnings(\"ignore\", category=sa_exc.SAWarning)\n",
    "\n",
    "DB_USER, DB_PASS = 'admin', 'Admin@1234Strong!'\n",
    "DB_HOST, DB_PORT = 'mcs1', '3306'\n",
    "TARGET_DB = 'lending_club'\n",
    "PASS_ENC = quote_plus(DB_PASS)\n",
    "FULL_URL = f\"mysql+pymysql://{DB_USER}:{PASS_ENC}@127.0.0.1:3306/lending_club\"\n",
    "\n",
    "engine = create_engine(FULL_URL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce24f4c-fc48-455c-a760-77893cff14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing:ðŸš€ Step 1: Executing ADVANCED SQL Feature Engineering...\n",
      "INFO:preprocessing:features created.\n",
      "INFO:preprocessing:âš™ï¸ Step 2: Scaling All Engineered Features...\n",
      "INFO:preprocessing: Data is now normalized and ready.\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering():\n",
    "    logger.info(\"Executing ADVANCED SQL Feature Engineering...\")\n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE loans_features AS\n",
    "    SELECT \n",
    "        target,\n",
    "        -- Ordinal Rank\n",
    "        CASE \n",
    "            WHEN grade = 'A' THEN 1 WHEN grade = 'B' THEN 2 \n",
    "            WHEN grade = 'C' THEN 3 WHEN grade = 'D' THEN 4 ELSE 5 \n",
    "        END AS grade_rank,\n",
    "        \n",
    "        LOG(1 + annual_inc) AS annual_inc_log,\n",
    "\n",
    "        -- Interaction 1: Risk Index (High Debt + High Interest)\n",
    "        (int_rate * dti) / 100 AS risk_index,\n",
    "\n",
    "        -- Interaction 2: Debt Burden (How much of annual income is the loan?)\n",
    "        (loan_amnt / (annual_inc + 1)) AS debt_burden,\n",
    "\n",
    "        CASE WHEN home_ownership = 'RENT' THEN 1 ELSE 0 END AS home_rent,\n",
    "        CASE WHEN home_ownership = 'MORTGAGE' THEN 1 ELSE 0 END AS home_mortgage,\n",
    "        CASE WHEN home_ownership = 'OWN' THEN 1 ELSE 0 END AS home_own,\n",
    "        \n",
    "        fico_score, int_rate, dti, emp_length_num\n",
    "    FROM loans_clean;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "        conn.execute(text(query))\n",
    "    logger.info(\"features created.\")\n",
    "\n",
    "def preprocessing():\n",
    "    logger.info(\"Scaling All Engineered Features...\")\n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE loans_processed AS\n",
    "    SELECT \n",
    "        target,\n",
    "        grade_rank, home_rent, home_mortgage, home_own,\n",
    "        (fico_score - 300) / (850 - 300) AS fico_scaled,\n",
    "        (risk_index - (SELECT MIN(risk_index) FROM loans_features)) / \n",
    "            ((SELECT MAX(risk_index) FROM loans_features) - (SELECT MIN(risk_index) FROM loans_features)) AS risk_scaled,\n",
    "        (debt_burden - (SELECT MIN(debt_burden) FROM loans_features)) / \n",
    "            ((SELECT MAX(debt_burden) FROM loans_features) - (SELECT MIN(debt_burden) FROM loans_features)) AS burden_scaled,\n",
    "        (int_rate / 35.0) AS rate_scaled,\n",
    "        (dti / 100.0) AS dti_scaled,\n",
    "        (annual_inc_log / 15.0) AS income_scaled\n",
    "    FROM loans_features;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "        conn.execute(text(query))\n",
    "    logger.info(\" Data is now normalized and ready.\")\n",
    "\n",
    "feature_engineering()\n",
    "preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8616bf3-4e88-4133-9115-bb81a4aa9663",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (284100533.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(result.keys())a\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# def verify_processed_data():\n",
    "#     print(\"\\n --- FINAL PROCESSED DATA PREVIEW (Top 5) ---\")\n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(text(\"SELECT * FROM loans_processed LIMIT 5\"))\n",
    "#         print(result.keys())a\n",
    "        \n",
    "#         for row in result:\n",
    "#             print(dict(row._mapping))\n",
    "\n",
    "# verify_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1f89f-515c-4777-aa91-ce9aeadafb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # target (0): This is your label. All 5 rows in the preview are people who paid back their loans.\n",
    "\n",
    "    # grade_rank (4, 3, 1, etc.): Your letters (A, B, C...) are now numbers. 4 represents Grade D, 1 represents Grade A.\n",
    "\n",
    "    # annual_inc_log (11.51, 10.71, etc.): These numbers are the result of \"squashing\" the income. Instead of seeing $100,000, the model sees 11.51. This prevents the model from being \"scared\" by huge numbers.\n",
    "\n",
    "    # loan_to_income_ratio (0.3, 0.88, etc.): This tells the model how heavy the debt is. 0.88 means the loan is 88% of their annual income (High Risk!).\n",
    "\n",
    "    # home_rent/mortgage/own (0 or 1): These are your \"switches.\" In row 1, home_mortgage: 1 tells the model this person has a mortgage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14482c14-716b-4fc9-b6d4-68d0839a0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def run_model_training():\n",
    "    logger.info(\"Model Training with Early Stopping...\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT * FROM loans_processed\"))\n",
    "        data = np.array(result.fetchall(), dtype=np.float32)\n",
    "        \n",
    "        total_paid = conn.execute(text(\"SELECT COUNT(*) FROM loans_processed WHERE target = 0\")).scalar()\n",
    "        total_default = conn.execute(text(\"SELECT COUNT(*) FROM loans_processed WHERE target = 1\")).scalar()\n",
    "        imbalance_ratio = total_paid / total_default\n",
    "\n",
    "    y = data[:, 0]\n",
    "    X = data[:, 1:]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    X_train_part, X_val, y_train_part, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=1000,       \n",
    "        max_depth=7,             \n",
    "        learning_rate=0.05,     \n",
    "        scale_pos_weight=imbalance_ratio,\n",
    "        eval_metric=['auc', 'logloss'],\n",
    "        early_stopping_rounds=15 \n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_part, y_train_part,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False \n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Training stopped at iteration: {model.best_iteration}\")\n",
    "    return model, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "123b870e-99f6-43c3-a0f2-944dcdb2844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing:Model Training with Early Stopping...\n",
      "INFO:preprocessing:Training stopped at iteration: 999\n",
      "INFO:preprocessing:ðŸ“Š Step 4: Optimizing Threshold and Evaluating...\n",
      "INFO:preprocessing:Best Threshold: 0.5134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL OPTIMIZED PERFORMANCE REPORT\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.66      0.75    208391\n",
      "         1.0       0.32      0.64      0.43     52331\n",
      "\n",
      "    accuracy                           0.65    260722\n",
      "   macro avg       0.60      0.65      0.59    260722\n",
      "weighted avg       0.77      0.65      0.69    260722\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing:Pipeline execution finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137299  71092]\n",
      " [ 18932  33399]]\n",
      "==================================================\n",
      "\n",
      "ðŸ’¡ TOP FEATURES BY WEIGHT:\n",
      "Feature 0: 0.8607\n",
      "Feature 1: 0.0259\n",
      "Feature 2: 0.0280\n",
      "Feature 3: 0.0088\n",
      "Feature 4: 0.0000\n",
      "Feature 5: 0.0141\n",
      "Feature 6: 0.0164\n",
      "Feature 7: 0.0242\n",
      "Feature 8: 0.0109\n",
      "Feature 9: 0.0109\n"
     ]
    }
   ],
   "source": [
    "def run_optimized_evaluation(model, X_test, y_test):\n",
    "    logger.info(\"ðŸ“Š Step 4: Optimizing Threshold and Evaluating...\")\n",
    "    \n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    \n",
    "    # F1 = 2 * (Prec * Rec) / (Prec + Rec)\n",
    "    f1_scores = (2 * precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    logger.info(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "    \n",
    "    y_pred_optimized = (y_probs >= best_threshold).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL OPTIMIZED PERFORMANCE REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, y_pred_optimized))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_optimized))\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "\n",
    "    print(\"\\nðŸ’¡ TOP FEATURES BY WEIGHT:\")\n",
    "    importances = model.feature_importances_\n",
    "    for i, v in enumerate(importances):\n",
    "        print(f\"Feature {i}: {v:.4f}\")\n",
    "\n",
    "    logger.info(\"Pipeline execution finished.\")\n",
    "\n",
    "trained_model, X_test_data, y_test_data = run_model_training()\n",
    "\n",
    "# 2. Evaluate\n",
    "run_optimized_evaluation(trained_model, X_test_data, y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06c5c61-17fd-42c1-9487-cd54d2008c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:Failed to extract font properties from /usr/share/fonts/truetype/noto/NotoColorEmoji.ttf: Can not load face (unknown file format; error code 0x2)\n",
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Setup Logging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sqlalchemy import create_engine, text\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# import logging\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Setup Logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(\"preprocessing\")\n",
    "\n",
    "# # Database Configuration\n",
    "# DB_USER, DB_PASS = 'admin', 'Admin@1234Strong!'\n",
    "# DB_HOST, DB_PORT = '127.0.0.1', '3306'\n",
    "# TARGET_DB = 'lending_club'\n",
    "# FULL_URL = f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{TARGET_DB}\"\n",
    "# engine = create_engine(FULL_URL)\n",
    "\n",
    "# # ==========================================\n",
    "# # Feature Engineering (Using SQL directly)\n",
    "# # ==========================================\n",
    "# def feature_engineering():\n",
    "#     logger.info(\"ðŸš€ Step 1: Executing Advanced SQL Feature Engineering...\")\n",
    "#     query = \"\"\"\n",
    "#     CREATE OR REPLACE TABLE loans_features AS\n",
    "#     SELECT \n",
    "#         target,\n",
    "#         -- Ordinal Rank\n",
    "#         CASE \n",
    "#             WHEN grade = 'A' THEN 1 WHEN grade = 'B' THEN 2 \n",
    "#             WHEN grade = 'C' THEN 3 WHEN grade = 'D' THEN 4 ELSE 5 \n",
    "#         END AS grade_rank,\n",
    "        \n",
    "#         LOG(1 + annual_inc) AS annual_inc_log,\n",
    "\n",
    "#         -- Interaction 1: Risk Index (High Debt + High Interest)\n",
    "#         (int_rate * dti) / 100 AS risk_index,\n",
    "\n",
    "#         -- Interaction 2: Debt Burden (How much of annual income is the loan?)\n",
    "#         (loan_amnt / (annual_inc + 1)) AS debt_burden,\n",
    "\n",
    "#         -- New Interaction: FICO Score * DTI\n",
    "#         (fico_score * dti) / 100 AS fico_dti_interaction,\n",
    "\n",
    "#         -- New Interaction: Loan Amount * Annual Income (scaled)\n",
    "#         (loan_amnt * LOG(1 + annual_inc)) AS loan_inc_interaction,\n",
    "\n",
    "#         CASE WHEN home_ownership = 'RENT' THEN 1 ELSE 0 END AS home_rent,\n",
    "#         CASE WHEN home_ownership = 'MORTGAGE' THEN 1 ELSE 0 END AS home_mortgage,\n",
    "#         CASE WHEN home_ownership = 'OWN' THEN 1 ELSE 0 END AS home_own,\n",
    "        \n",
    "#         fico_score, int_rate, dti, emp_length_num\n",
    "#     FROM loans_clean;\n",
    "#     \"\"\"\n",
    "#     with engine.connect() as conn:\n",
    "#         conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "#         conn.execute(text(query))\n",
    "#     logger.info(\"âœ… Advanced features created.\")\n",
    "\n",
    "# # ==========================================\n",
    "# # Data Preprocessing (Scaling and Normalizing Features)\n",
    "# # ==========================================\n",
    "# def preprocessing():\n",
    "#     logger.info(\"âš™ï¸ Step 2: Scaling All Engineered Features...\")\n",
    "#     query = \"\"\"\n",
    "#     CREATE OR REPLACE TABLE loans_processed AS\n",
    "#     SELECT \n",
    "#         target,\n",
    "#         grade_rank, home_rent, home_mortgage, home_own,\n",
    "#         (fico_score - 300) / (850 - 300) AS fico_scaled,\n",
    "#         (risk_index - (SELECT MIN(risk_index) FROM loans_features)) / \n",
    "#             ((SELECT MAX(risk_index) FROM loans_features) - (SELECT MIN(risk_index) FROM loans_features)) AS risk_scaled,\n",
    "#         (debt_burden - (SELECT MIN(debt_burden) FROM loans_features)) / \n",
    "#             ((SELECT MAX(debt_burden) FROM loans_features) - (SELECT MIN(debt_burden) FROM loans_features)) AS burden_scaled,\n",
    "#         (fico_dti_interaction - (SELECT MIN(fico_dti_interaction) FROM loans_features)) / \n",
    "#             ((SELECT MAX(fico_dti_interaction) FROM loans_features) - (SELECT MIN(fico_dti_interaction) FROM loans_features)) AS fico_dti_scaled,\n",
    "#         (loan_inc_interaction - (SELECT MIN(loan_inc_interaction) FROM loans_features)) / \n",
    "#             ((SELECT MAX(loan_inc_interaction) FROM loans_features) - (SELECT MIN(loan_inc_interaction) FROM loans_features)) AS loan_inc_scaled,\n",
    "#         (int_rate / 35.0) AS rate_scaled,\n",
    "#         (dti / 100.0) AS dti_scaled,\n",
    "#         (annual_inc_log / 15.0) AS income_scaled\n",
    "#     FROM loans_features;\n",
    "#     \"\"\"\n",
    "#     with engine.connect() as conn:\n",
    "#         conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "#         conn.execute(text(query))\n",
    "#     logger.info(\"âœ… Data is now normalized and ready.\")\n",
    "\n",
    "# # ==========================================\n",
    "# # Verify Processed Data (Using SQL directly)\n",
    "# # ==========================================\n",
    "# def verify_processed_data():\n",
    "#     logger.info(\"ðŸ“Š --- FINAL PROCESSED DATA PREVIEW (Top 5) ---\")\n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(text(\"SELECT * FROM loans_processed LIMIT 5\"))\n",
    "#         for row in result:\n",
    "#             print(dict(row))\n",
    "\n",
    "# verify_processed_data()\n",
    "\n",
    "# # ==========================================\n",
    "# # Train Model with Hyperparameter Tuning (GridSearchCV)\n",
    "# # ==========================================\n",
    "# def run_model_training():\n",
    "#     logger.info(\"ðŸ‹ï¸ Step 3: Starting Advanced Model Training with Early Stopping and Hyperparameter Tuning...\")\n",
    "    \n",
    "#     # Fetch processed data from the database\n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(text(\"SELECT * FROM loans_processed\"))\n",
    "#         data = np.array(result.fetchall(), dtype=np.float32)\n",
    "    \n",
    "#     y = data[:, 0]\n",
    "#     X = data[:, 1:]\n",
    "\n",
    "#     # 1. Train-Test Split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.2, random_state=42, stratify=y\n",
    "#     )\n",
    "\n",
    "#     # 2. SMOTE for Handling Imbalance\n",
    "#     smote = SMOTE(sampling_strategy='minority')\n",
    "#     X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#     # 3. Grid Search for Hyperparameter Tuning\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [500, 1000],\n",
    "#         'max_depth': [3, 5, 7],\n",
    "#         'learning_rate': [0.01, 0.05, 0.1],\n",
    "#         'subsample': [0.8, 1.0],\n",
    "#         'colsample_bytree': [0.8, 1.0]\n",
    "#     }\n",
    "\n",
    "#     model = xgb.XGBClassifier(scale_pos_weight=1.5, eval_metric=['auc', 'logloss'], early_stopping_rounds=15)\n",
    "#     grid_search = GridSearchCV(model, param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "#     logger.info(f\"âœ… Best Model Found: {grid_search.best_params_}\")\n",
    "#     return grid_search.best_estimator_, X_test, y_test\n",
    "\n",
    "# # ==========================================\n",
    "# # Evaluation and Optimized Threshold\n",
    "# # ==========================================\n",
    "# def run_optimized_evaluation(model, X_test, y_test):\n",
    "#     logger.info(\"ðŸ“Š Step 4: Optimizing Threshold and Evaluating...\")\n",
    "    \n",
    "#     # Get probabilities (continuous predictions)\n",
    "#     y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#     # Precision-Recall Curve\n",
    "#     precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "#     # Find the best threshold for F1-Score\n",
    "#     f1_scores = (2 * precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "#     best_idx = np.argmax(f1_scores)\n",
    "#     best_threshold = thresholds[best_idx]\n",
    "#     logger.info(f\"âœ¨ Best Threshold: {best_threshold:.4f}\")\n",
    "    \n",
    "#     # Apply optimized threshold\n",
    "#     y_pred_optimized = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "#     # Classification Report & Confusion Matrix\n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"ðŸ“ˆ FINAL OPTIMIZED PERFORMANCE REPORT\")\n",
    "#     print(\"=\"*50)\n",
    "#     print(classification_report(y_test, y_pred_optimized))\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion_matrix(y_test, y_pred_optimized))\n",
    "#     print(\"=\"*50)\n",
    "\n",
    "#     # ROC AUC and PR AUC\n",
    "#     roc_auc = roc_auc_score(y_test, y_probs)\n",
    "#     pr_auc = average_precision_score(y_test, y_probs)\n",
    "#     print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "#     print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "#     # Feature Importance Visualization\n",
    "#     importances = model.feature_importances_\n",
    "#     indices = np.argsort(importances)[::-1]\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.title(\"Top Features by Weight\")\n",
    "#     plt.barh(range(10), importances[indices][:10], align=\"center\")\n",
    "#     plt.yticks(range(10), [f\"Feature {i}\" for i in indices[:10]])\n",
    "#     plt.xlabel(\"Feature Importance\")\n",
    "#     plt.show()\n",
    "\n",
    "# # ==========================================\n",
    "# # Run the Workflow\n",
    "# # ==========================================\n",
    "# trained_model, X_test_data, y_test_data = run_model_training()\n",
    "\n",
    "# # 2. Evaluate\n",
    "# run_optimized_evaluation(trained_model, X_test_data, y_test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
